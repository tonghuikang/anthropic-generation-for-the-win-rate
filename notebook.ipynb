{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cc57a9",
   "metadata": {},
   "source": [
    "# Optimize a prompt that maximizes win rate\n",
    "\n",
    "Given\n",
    "- a certified judge classifier `JUDGE_CLASSIFICATION_PROMPT: str`\n",
    "- some input string data samples `INPUT_DATA_SAMPLES: list[str]`\n",
    "- initial prompt `INITIAL_GENERATION_PROMPT: str`\n",
    "\n",
    "Produce\n",
    "- a prompt with a good winrate (measured by the judge) against other prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b045f",
   "metadata": {},
   "source": [
    "## Example use case presented in this notebook\n",
    "\n",
    "Given a Quora answer, write followup questions to the answer.\n",
    "\n",
    "Great followup questions should be appealing to respond to and answers should be appealing to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caac65",
   "metadata": {},
   "source": [
    "## Foreword\n",
    "\n",
    "Think of how the project manager interacts with the prompt engineer.\n",
    "\n",
    "The project manager does not need to provide examples of good output, just the criteria of what good output should be.\n",
    "\n",
    "Inspirations\n",
    "\n",
    "- Anthropic workbench https://console.anthropic.com/workbench/\n",
    "- Cohere prompt tuner https://cohere.com/blog/intro-prompt-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0fca9",
   "metadata": {},
   "source": [
    "## Frequently asked questions\n",
    "\n",
    "Why don't you use the judge prompt in the generation prompt?\n",
    "- You can if you want. I expect models in the future to be iteratively think of the best reponse by iterative brainstorming responses and think which is the response is the best. The idea of prompt engineering is to get the model to learn the shortcut (output tokens are not free).\n",
    "\n",
    "Why don't you tune the `JUDGE_CLASSIFICATION_PROMPT`?\n",
    "- In this tool we assume that we trust `JUDGE_CLASSIFICATION_PROMPT`. You should tune this elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a685ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "import anthropic\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418884bc",
   "metadata": {},
   "source": [
    "# Inputs for the tool user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10626a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_question_and_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How would you rate the success of the Olympic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is more of a pure talent, Lebron or Curry?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          source_question_and_answer\n",
       "0  How would you rate the success of the Olympic ...\n",
       "1  Who is more of a pure talent, Lebron or Curry?..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"source_questions_and_answers.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2999efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_SAMPLES: list[str] = list(df[\"source_question_and_answer\"])\n",
    "source_questions_and_answers = INPUT_DATA_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538f7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_GENERATION_PROMPT = \"\"\"\n",
    "You are given the following source question and a source answer.\n",
    "\n",
    "<source_question_and_answer>\n",
    "{source_question_and_answer}\n",
    "</source_question_and_answer>\n",
    "\n",
    "Write between 3 to 5 followup questions to the answer.\n",
    "\n",
    "Return each question between <question> and </question>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf8021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_CLASSIFICATION_PROMPT = \"\"\"\n",
    "You are given an answer and two sets of followup questions.\n",
    "\n",
    "Determine which set of followup questions is better.\n",
    "\n",
    "The better set of followup questions should have these characteristics\n",
    "- Each question could be understood without context\n",
    "- Each question should be written concisely\n",
    "- Each question should appear between <question> and </question>\n",
    "- There should be between 3 to 5 questions.\n",
    "- Each question should be distinct.\n",
    "\n",
    "This is the source question and source answer\n",
    "<source_question_and_answer>\n",
    "{source_question_and_answer}\n",
    "</source_question_and_answer>\n",
    "\n",
    "This is the first set of followup questions\n",
    "\n",
    "{followup_questions_one}\n",
    "\n",
    "This is the second set of followup questions\n",
    "\n",
    "{followup_questions_two}\n",
    "\n",
    "Write some concise reasoning, end your response with one for the follow\n",
    "- Set <label>one</label> is better.\n",
    "- Set <label>two</label> is better.\n",
    "- Both sets <label>tie</label>.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f473196",
   "metadata": {},
   "source": [
    "# Judge classification prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e326505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that we trust this prompt\n",
    "judge_classification_prompt = JUDGE_CLASSIFICATION_PROMPT\n",
    "\n",
    "def judge_classification(\n",
    "    source_question_and_answer: str,\n",
    "    followup_questions_one: str,\n",
    "    followup_questions_two: str,\n",
    ") -> tuple[str, str]:\n",
    "    # return either one, two or tie\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": judge_classification_prompt.format(\n",
    "                    source_question_and_answer=source_question_and_answer,\n",
    "                    followup_questions_one=followup_questions_one,\n",
    "                    followup_questions_two=followup_questions_two,\n",
    "                )\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    message_text = message.content[0].text\n",
    "    if \"<label>one</label>\" in message_text:\n",
    "        return message_text, \"one\"\n",
    "    if \"<label>one</label>\" in message_text:\n",
    "        return message_text, \"two\"\n",
    "    return message_text, \"tie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405c66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_question_and_answer = \"\"\"\n",
    "Who is more of a pure talent, Lebron or Curry?\n",
    "\n",
    "I saw a recent interview with Team USA, where basketball players shared when they first dunked. Some guys dunked at 11 or 12. A few were “late bloomers” who didn’t dunk until 17. Steph didn’t dunk until he was 19 years old.\n",
    "\n",
    "This kid’s never survived on talent. Never. He’s 6′3″ with shoes, four inches shorter than the NBA average. He’s got a solid vertical (35.5″) but would never have made it to the NBA if not for his shooting ability. His height plus his less “glamorous” college career made him a risky first-round pick. The Warriors grabbed him with the 7th pick. They had no idea\n",
    "\n",
    "Compare that to LeBron.\n",
    "\n",
    "He was a 6′7″ 16-year-old and unstoppable in high school. His numbers were unreal—not just the scoring, either, but the rebounding and passing, too. LeBron clocked a vertical north of 40 inches, more than enough to dunk on anybody in the league. The King didn’t need to really workshop his shooting until several years into his career. That’s talent.\n",
    "\n",
    "I mean no disrespect to either player. Steph’s still very talented and LeBron still put a LOT of work in. But if they both won the genetic lottery, Steph won $10 million and LeBron won $100 million. Guys like Steph happen. They just rarely have the same shooting foundation (thanks to an NBA father) and work ethic to pull off his career. Guys like LeBron are much rarer. But neither could have survived with just talent or skill. That’s no longer possible in the league.\n",
    "\"\"\".strip()\n",
    "\n",
    "followup_questions_one = \"\"\"\n",
    "<question>How do basketball players with high vertical leaps compare to those with less-than-average verticals?</question>\n",
    "\n",
    "<question>Is work ethic more important than natural talent in basketball?</question>\n",
    "\n",
    "<question>What are some examples of athletes who overcame physical limitations to achieve success?</question>\n",
    "\n",
    "<question>How important is it for basketball players to have a solid foundation in shooting?</question>\n",
    "\n",
    "<question>How has the role of talent and skill evolved in the NBA over time?</question>\n",
    "\"\"\"\n",
    "\n",
    "followup_questions_two = \"\"\"\n",
    "1. <question>How has the balance between natural talent and skill development evolved in the NBA over the past few decades, and are there other examples that illustrate this shift?</question>\n",
    "\n",
    "2. <question>You mentioned Curry's NBA father providing a shooting foundation - can you elaborate on other successful NBA players who had to overcome physical limitations through specialized skills passed down from family members?</question>\n",
    "\n",
    "3. <question>In your genetic lottery analogy, where would you place other current NBA superstars on the spectrum between Curry ($10M) and LeBron ($100M), and why?</question>\n",
    "\n",
    "4. <question>Given the contrast between early and late physical developers in basketball, what other \"late bloomers\" besides Curry have gone on to achieve NBA success despite not being early athletic standouts?</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e584b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "justification, judgement = judge_classification(\n",
    "    source_question_and_answer,\n",
    "    followup_questions_one,\n",
    "    followup_questions_two,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aab8a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7f5360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze both sets:\n",
      "\n",
      "Set 1:\n",
      "- Questions are concise and clear\n",
      "- Each can stand alone without context\n",
      "- Covers broad but related topics\n",
      "- Has exactly 5 questions\n",
      "- Easy to understand and answer\n",
      "\n",
      "Set 2:\n",
      "- Questions are lengthy and compound\n",
      "- Requires context from the original answer\n",
      "- Some questions are too specific (e.g., the genetic lottery analogy)\n",
      "- Has 4 questions\n",
      "- Multiple questions within single questions\n",
      "\n",
      "While both sets explore interesting angles, Set 1 is superior because:\n",
      "1. It maintains simplicity while covering important topics\n",
      "2. Questions are independent and don't require original context\n",
      "3. Each question focuses on one specific aspect\n",
      "4. The format is clean and consistent\n",
      "5. Follows all the given criteria perfectly\n",
      "\n",
      "Set <label>one</label> is better.\n"
     ]
    }
   ],
   "source": [
    "print(justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df8109",
   "metadata": {},
   "source": [
    "# Generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941445ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_generation_prompt = INITIAL_GENERATION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30cfd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(\n",
    "    generation_prompt: str,\n",
    "    source_question_and_answer: str,\n",
    ") -> str:\n",
    "    try:\n",
    "        generation_prompt_with_inputs = generation_prompt.format(\n",
    "            source_question_and_answer=source_question_and_answer,\n",
    "        )\n",
    "    except:\n",
    "        return \"{source_question_and_answer} should appear in the prompt\"\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": generation_prompt_with_inputs\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    message_text = message.content[0].text\n",
    "    return message_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae44c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_questions = generation(initial_generation_prompt, source_question_and_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c17f922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<question>How much did having an NBA player as a father (Dell Curry) influence Steph's development as a shooter compared to players who didn't have that advantage?</question>\n",
      "\n",
      "<question>If LeBron was clearly the more naturally gifted athlete, what specific skills or aspects of his game did he have to work hardest to develop throughout his career?</question>\n",
      "\n",
      "<question>How common is it for NBA players to make it to the league primarily based on their shooting ability like Curry, rather than their athletic attributes?</question>\n",
      "\n",
      "<question>What would you consider the minimum combination of natural talent and developed skills required to succeed in today's NBA compared to previous eras?</question>\n"
     ]
    }
   ],
   "source": [
    "print(followup_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb369c",
   "metadata": {},
   "source": [
    "# Win rate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65585c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def calculate_winrate(\n",
    "    source_questions_and_answers,\n",
    "    generation_prompt_one,\n",
    "    generation_prompt_two\n",
    "):\n",
    "    one_win = 0 \n",
    "    two_win = 0\n",
    "\n",
    "    one_wins_judgements = []\n",
    "    one_loses_judgements = []\n",
    "    two_wins_judgements = []\n",
    "    two_loses_judgements = []\n",
    "    one_tie_judgements = []\n",
    "    two_tie_judgements = []\n",
    "    \n",
    "    def calculate_winrate_single(source_question_and_answer):\n",
    "        followup_questions_one = generation(\n",
    "            generation_prompt=generation_prompt_one,\n",
    "            source_question_and_answer=source_question_and_answer,\n",
    "        )\n",
    "        followup_questions_two = generation(\n",
    "            generation_prompt=generation_prompt_two,\n",
    "            source_question_and_answer=source_question_and_answer,\n",
    "        )\n",
    "        justification, judgement = judge_classification(\n",
    "            source_question_and_answer=source_question_and_answer,\n",
    "            followup_questions_one=followup_questions_one,\n",
    "            followup_questions_two=followup_questions_two,\n",
    "        )\n",
    "        return justification, judgement\n",
    "        \n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        results = executor.map(calculate_winrate_single, source_questions_and_answers)\n",
    "        results = list(results)    \n",
    "    \n",
    "    for justification, judgement in results:\n",
    "        if judgement == \"one\":\n",
    "            one_win += 1\n",
    "            one_wins_judgements.append(justification)\n",
    "            two_loses_judgements.append(justification)\n",
    "        elif judgement == \"two\":\n",
    "            two_win += 1\n",
    "            two_wins_judgements.append(justification)\n",
    "            one_loses_judgements.append(justification)\n",
    "        else:\n",
    "            one_win += 1/2\n",
    "            two_win += 1/2\n",
    "            one_tie_judgements.append(justification)\n",
    "            two_tie_judgements.append(justification)\n",
    "    return (\n",
    "        (\n",
    "            one_win / (one_win + two_win),\n",
    "            one_wins_judgements,\n",
    "            one_tie_judgements,\n",
    "            one_loses_judgements,\n",
    "        ),\n",
    "        (\n",
    "            two_win / (one_win + two_win),\n",
    "            two_wins_judgements,\n",
    "            two_tie_judgements,\n",
    "            two_loses_judgements,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92be4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_one, evaluation_two = calculate_winrate(\n",
    "    source_questions_and_answers = source_questions_and_answers,\n",
    "    generation_prompt_one = initial_generation_prompt,\n",
    "    generation_prompt_two = initial_generation_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0adf788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65, ['Let me analyze both sets:\\n\\nSet 1:\\n- Questions are more concise\\n- Questions can stand alone without context\\n- Questions avoid unnecessary words like \"you mention\" or \"that people\"\\n- Questions maintain professional tone\\n- All questions are distinct and relevant\\n- Format follows the requested structure\\n- Has 4 questions (within 3-5 range)\\n\\nSet 2:\\n- Questions contain extra words that don\\'t add value\\n- Some questions include contextual phrases like \"When you mention\"\\n- Questio\n"
     ]
    }
   ],
   "source": [
    "print(str(evaluation_one)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25ac9f",
   "metadata": {},
   "source": [
    "# Prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8faa637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_string_template = \"\"\"\n",
    "Winrate: {winrate}\n",
    "Cases where the prompt won: {win_judgements}\n",
    "Cases where the prompt ties: {tie_judgements}\n",
    "Cases where the prompt loses: {lose_judgements}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84c23f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_prompt_template = \"\"\"\n",
    "Improve the generation prompt according to the feedback\n",
    "\n",
    "{generation_prompt}\n",
    "\n",
    "{evaluation_string}\n",
    "\n",
    "Return the new prompt between <prompt> and </prompt>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11fbc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(evaluation, generation_prompt) -> str:\n",
    "    winrate, win_judgements, tie_judgements, lose_judgements = evaluation\n",
    "    evaluation_string = evaluation_string_template.format(\n",
    "        winrate=winrate,\n",
    "        win_judgements=win_judgements,\n",
    "        tie_judgements=tie_judgements,\n",
    "        lose_judgements=lose_judgements,\n",
    "    )\n",
    "    optimization_prompt = optimization_prompt_template.format(\n",
    "        generation_prompt=generation_prompt,\n",
    "        evaluation_string=evaluation_string,        \n",
    "    )\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": optimization_prompt\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    optimization_response = message.content[0].text\n",
    "    optimized_prompt = extract_from_tags(optimization_response, tag_string=\"prompt\")\n",
    "    \n",
    "    return optimization_response, optimized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d332bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_from_tags(text, tag_string=\"prompt\"):\n",
    "    pattern = f'<{tag_string}>(.*?)</{tag_string}>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c47a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_response, optimized_prompt = optimization(evaluation_one, initial_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9201f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the analysis of winning and tie cases, I'll create an improved prompt that emphasizes conciseness, specificity, and standalone clarity:\n",
      "\n",
      "<prompt>\n",
      "You are given the following source question and a source answer.\n",
      "\n",
      "<source_question_and_answer>\n",
      "{source_question_and_answer}\n",
      "</source_question_and\n"
     ]
    }
   ],
   "source": [
    "print(optimization_response[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f93da819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given the following source question and a source answer.\n",
      "\n",
      "<source_question_and_answer>\n",
      "{source_question_and_answer}\n",
      "</source_question_and_answer>\n",
      "\n",
      "Write between 3 to 5 followup questions to the answer following these criteria:\n",
      "- Make each question concise and direct\n",
      "- Ensure each question can stand alone without context\n",
      "- Focus on specific details rather than broad speculation\n",
      "- Avoid phrases like \"you mention\" or \"could you elaborate\"\n",
      "- Maintain distinct topics for each question\n",
      "- Include both theoretical and practical aspects where relevant\n",
      "\n",
      "Return each question between <question> and </question>\n"
     ]
    }
   ],
   "source": [
    "print(optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761f75b",
   "metadata": {},
   "source": [
    "# Iterative optimization\n",
    "\n",
    "Initial prompt `inital_generation_prompt`\n",
    "\n",
    "-> 2x generation-one prompts `generation_prompt_one_{1,2}`\n",
    "\n",
    "-> 4x generation-two prompts `generation_prompt_two_{1,2,3,4}`\n",
    "\n",
    "We show the win rate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fcc2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_response, generation_prompt_one = optimization(evaluation_one, initial_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2ea7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_response, generation_prompt_two = optimization(evaluation_one, initial_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a20b6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given the following source question and a source answer.\n",
      "\n",
      "<source_question_and_answer>\n",
      "{source_question_and_answer}\n",
      "</source_question_and_answer>\n",
      "\n",
      "Generate 3 to 5 followup questions to the answer. Each question should:\n",
      "- Be concise and direct\n",
      "- Focus on a single aspect or concept\n",
      "- Stand alone without requiring context\n",
      "- Avoid phrases like \"you mention\" or \"could you elaborate\"\n",
      "- Build naturally from specific points in the source\n",
      "- Ask for specific details rather than general opinions\n",
      "\n",
      "Return each question between <question> and </question>\n"
     ]
    }
   ],
   "source": [
    "print(generation_prompt_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3048ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given the following source question and a source answer.\n",
      "\n",
      "<source_question_and_answer>\n",
      "{source_question_and_answer}\n",
      "</source_question_and_answer>\n",
      "\n",
      "Write between 3 to 5 followup questions to the answer. Follow these guidelines:\n",
      "1. Write concise questions that avoid unnecessary words\n",
      "2. Ensure each question can be understood without context\n",
      "3. Make each question focus on a single, distinct aspect\n",
      "4. Include both specific details and broader implications\n",
      "5. Build questions that naturally flow from the source material\n",
      "\n",
      "Return each question between <question> and </question>\n"
     ]
    }
   ],
   "source": [
    "print(generation_prompt_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85f2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
