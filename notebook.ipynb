{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cc57a9",
   "metadata": {},
   "source": [
    "# Convert your judge into a generation prompt\n",
    "\n",
    "Given\n",
    "- a certified judge classifier `JUDGE_CLASSIFICATION_PROMPT: str`\n",
    "- some input string data samples `INPUT_CONTENTS: tuple[str]`\n",
    "- initial prompt `INITIAL_GENERATION_PROMPT: str`\n",
    "\n",
    "Produce\n",
    "- a prompt with a good winrate (measured by the judge) against other prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffeaa65",
   "metadata": {},
   "source": [
    "## Foreword\n",
    "\n",
    "If you can't verify what is good or what is bad, you can't make good generations.\n",
    "\n",
    "But if you can verify, you should easily be able to make good generations - and this tool helps you write this prompt.\n",
    "\n",
    "\n",
    "#### Inspirations\n",
    "\n",
    "- Cohere prompt tuner https://cohere.com/blog/intro-prompt-tuner\n",
    "- Anthropic workbench https://console.anthropic.com/workbench/\n",
    "- Chatbot Arena leaderboard https://lmarena.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0aacb",
   "metadata": {},
   "source": [
    "## Example use case presented in this notebook\n",
    "\n",
    "Given a Quora answer, write followup questions to the answer.\n",
    "\n",
    "Great followup questions should be appealing to respond to and answers should be appealing to read.\n",
    "\n",
    "The inputs for this use case\n",
    "- `JUDGE_CLASSIFICATION_PROMPT: str` - prompt that classifies whether one set of output is better than the other\n",
    "- `process_response_for_judgement: func[str] -> str` - how the output is post-processed for judgement\n",
    "- `INPUT_CONTENTS: tuple[str]` - Quora answers\n",
    "- `INITIAL_GENERATION_PROMPT: str` - prompt that generates followup questions\n",
    "\n",
    "Produce\n",
    "- a prompt with a good winrate (measured by the judge) against other prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93eac0",
   "metadata": {},
   "source": [
    "## Frequently asked questions\n",
    "\n",
    "Why don't you include prompt for `JUDGE_CLASSIFICATION` in the generation prompt?\n",
    "- You can if you want. But the result still needs to perform better according to the judge.\n",
    "- I expect models in the future to be brainstorming responses and then think carefully which is the response is the best. However, doing so will incur extra cost and latency, and we might not want this tradeoff.\n",
    "- The idea of prompt engineering for the generation prompt is to teach the model shortcuts on what good outputs are.\n",
    "\n",
    "Why don't you tune the `JUDGE_CLASSIFICATION` as well?\n",
    "- In this tool we assume that we trust `JUDGE_CLASSIFICATION`.\n",
    "- It is important to get this right. You should tune this, and tune this elsewhere.\n",
    "- In practice we do want to provide feedback to point out where the judge is obviously wrong. I leave this to the roadmap.\n",
    "\n",
    "Why is the `JUDGE_CLASSIFICATION` doing comparison of two generation outputs instead of classifying whether one is good?\n",
    "- For some answers it is easy to generate good followup questions but tricky to generate great followup questions.\n",
    "- In other answers it may be even difficult to generate followup questions that are not outright bad.\n",
    "- I want the judge to be useful in both cases.\n",
    "\n",
    "Why is `judge_classification` is also an input to this tool?\n",
    "- In my prompt, I ask the output to include a rationale along with the text we show to the user.\n",
    "- I don't want the rationale to bias the judgement of text we show to the user.\n",
    "- You might need different post-processing methods for different use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79cf20a",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Print more intermediate results to html\n",
    "- Improve the optimization prompt\n",
    "- Make the optimization prompt write all of system prompt, user prompt and assistant prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5054494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "import anthropic\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad7bf3",
   "metadata": {},
   "source": [
    "# Inputs for the tool user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv(\"content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36794536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why do AI image generators have so much trouble rendering hands?\\n\\nTong Hui Kang\\nMachine Learning Engineer in Recommendations\\nThe current AI image generators are not rewarded for drawing hands well.\\nThe training input for image models is a huge set of images and their captions. The objective here is to fit a function f(caption) -&gt; image, by minimizing some loss function L(caption, image).\\nL(caption, image) is high when the image is unrelated to the caption, and L(caption, image) is low when the image is very related to the caption. The image generation model is trained to minimize L(caption, image).\\nHowever, L(caption, image) might be high for images with deformities, because L(caption, image) might only measure whether the image is related to the caption.\\nIf you don’t penalize deformities in the training, you can’t generate images free-of-deformities. As of my current understanding, generating images without deformities is still an open question.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At what point do you consider an LLM to have an ability to plan?\\n\\nTong Hui Kang\\nMachine Learning Engineer in Recommendations\\nConsider the following query and response\\nUser:\\nHow many r are there in strawberry? Check your answer\\nAssistant:\\nThere are 2 'r' letters in the word \"strawberry\".\\nTo check this answer, let's break down the word:\\ns-t-r-a-w-b-e-r-r-y\\nWe can see that there are indeed two instances of the letter 'r' in \"strawberry\":\\n1. The first 'r' comes after 't'\\n2. The second 'r' is the second-to-last letter\\nThis confirms that the answer of 2 'r' letters is correct.\\n\\nhttps://poe.com/s/X6Rkh9wDJ50PSc...\\nThe LLM has planned to BS their justification when they print the number “2”.\\nBad planning is still planning.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    content\n",
       "0  Why do AI image generators have so much trouble rendering hands?\\n\\nTong Hui Kang\\nMachine Learning Engineer in Recommendations\\nThe current AI image generators are not rewarded for drawing hands well.\\nThe training input for image models is a huge set of images and their captions. The objective here is to fit a function f(caption) -> image, by minimizing some loss function L(caption, image).\\nL(caption, image) is high when the image is unrelated to the caption, and L(caption, image) is low when the image is very related to the caption. The image generation model is trained to minimize L(caption, image).\\nHowever, L(caption, image) might be high for images with deformities, because L(caption, image) might only measure whether the image is related to the caption.\\nIf you don’t penalize deformities in the training, you can’t generate images free-of-deformities. As of my current understanding, generating images without deformities is still an open question.\n",
       "1                                                                                                                                                                                                                                    At what point do you consider an LLM to have an ability to plan?\\n\\nTong Hui Kang\\nMachine Learning Engineer in Recommendations\\nConsider the following query and response\\nUser:\\nHow many r are there in strawberry? Check your answer\\nAssistant:\\nThere are 2 'r' letters in the word \"strawberry\".\\nTo check this answer, let's break down the word:\\ns-t-r-a-w-b-e-r-r-y\\nWe can see that there are indeed two instances of the letter 'r' in \"strawberry\":\\n1. The first 'r' comes after 't'\\n2. The second 'r' is the second-to-last letter\\nThis confirms that the answer of 2 'r' letters is correct.\\n\\nhttps://poe.com/s/X6Rkh9wDJ50PSc...\\nThe LLM has planned to BS their justification when they print the number “2”.\\nBad planning is still planning."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301bbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CONTENTS: tuple[str] = tuple(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af58c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_GENERATION_PROMPT = \"\"\"\n",
    "This is a Quora answer. Write 5 followup questions to this answer.\n",
    "\n",
    "Requirements\n",
    "\n",
    "- Questions asked should not require context to be understood.\n",
    "- The questions should not use more words than necessary.\n",
    "- These questions are questions that the author would answer and people would read.\n",
    "\n",
    "\n",
    "Reply in this format\n",
    "\n",
    "<item>\n",
    "<rationale>\n",
    "rationale\n",
    "</rationale>\n",
    "<question>\n",
    "question\n",
    "</question>\n",
    "</item>\n",
    "...\n",
    "\n",
    "\n",
    "This is the question and answer.\n",
    "\n",
    "<source_question_and_answer>\n",
    "{input_content}\n",
    "</source_question_and_answer>\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2400fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_CLASSIFICATION_PROMPT = \"\"\"\n",
    "You are given an answer and two sets of followup questions.\n",
    "\n",
    "Determine which set of followup questions is better.\n",
    "\n",
    "These are required\n",
    "- Each question should appear between <question> and </question>\n",
    "- There should be 5 questions\n",
    "\n",
    "The followup questions are considered bad if they have any of these characteristics\n",
    "- There ambiguous references in the question that cannot be understood without context\n",
    "- The question is using extra words than necessary\n",
    "\n",
    "If both set of questions have neither of the bad characteristics, the better set of followup questions should have these characteristics\n",
    "- Each question should be distinct\n",
    "- The questions ask for knowledge that is not easily found online\n",
    "- The author is likely to answer the questions\n",
    "- People interested in the source question and answer will also be interested in answers to the followup questions\n",
    "\n",
    "This is the source question and source answer\n",
    "<source_question_and_answer>\n",
    "{input_content}\n",
    "</source_question_and_answer>\n",
    "\n",
    "This is the first set of followup questions\n",
    "\n",
    "{response_one}\n",
    "\n",
    "This is the second set of followup questions\n",
    "\n",
    "{response_two}\n",
    "\n",
    "Write some reasoning, end your response with one for the following\n",
    "- Set <label>one</label> is better.\n",
    "- Set <label>two</label> is better.\n",
    "- Both sets <label>tie</label>.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323e45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_response_for_judgement(response):\n",
    "    pattern = r'<question>(.*?)</question>'\n",
    "    return \"\\n\\n\".join(re.findall(pattern, response, re.DOTALL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fd75c",
   "metadata": {},
   "source": [
    "# Judge classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e326505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that we trust this prompt\n",
    "\n",
    "def judge_classification(\n",
    "    input_content: str,\n",
    "    response_one: str,\n",
    "    response_two: str,\n",
    ") -> tuple[str, str]:\n",
    "    # return either one, two or tie\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": JUDGE_CLASSIFICATION_PROMPT.format(\n",
    "                    input_content=input_content,\n",
    "                    response_one=process_response_for_judgement(response_one),\n",
    "                    response_two=process_response_for_judgement(response_two),\n",
    "                )\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    message_text = message.content[0].text\n",
    "    if \"<label>one</label>\" in message_text:\n",
    "        return message_text, \"one\"\n",
    "    if \"<label>two</label>\" in message_text:\n",
    "        return message_text, \"two\"\n",
    "    return message_text, \"tie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405c66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_content = \"\"\"\n",
    "Why do AI image generators have so much trouble rendering hands?\n",
    "\n",
    "Tong Hui Kang\n",
    "Machine Learning Engineer in Recommendations\n",
    "The current AI image generators are not rewarded for drawing hands well.\n",
    "The training input for image models is a huge set of images and their captions. The objective here is to fit a function f(caption) -> image, by minimizing some loss function L(caption, image).\n",
    "L(caption, image) is high when the image is unrelated to the caption, and L(caption, image) is low when the image is very related to the caption. The image generation model is trained to minimize L(caption, image).\n",
    "However, L(caption, image) might be high for images with deformities, because L(caption, image) might only measure whether the image is related to the caption.\n",
    "If you don’t penalize deformities in the training, you can’t generate images free-of-deformities. As of my current understanding, generating images without deformities is still an open question.\n",
    "\"\"\".strip()\n",
    "\n",
    "response_one = \"\"\"\n",
    "<question>What are some AI image generation models?</question>\n",
    "\n",
    "<question>Where can I find the training set for the image models?</question>\n",
    "\n",
    "<question>What are some ways to improve the quality of AI-generated images?</question>\n",
    "\n",
    "<question>What is the role of captions in AI image generation?</question>\n",
    "\n",
    "<question>What techniques have researchers tried so far to address the hand rendering problem?</question>\n",
    "\"\"\"\n",
    "\n",
    "response_two = \"\"\"\n",
    "<question>How do we reward AI image generators to draw hands correction?</question>\n",
    "\n",
    "<question>Why are hands particularly challenging compared to other anatomical features?</question>\n",
    "\n",
    "<question>What preprocessing techniques have worked best for curating training datasets that lead to better hand renderings?</question>\n",
    "\n",
    "<question>What specific loss functions have you found most effective for detecting and penalizing anatomical deformities in AI-generated images?</question>\n",
    "\n",
    "<question>How do different AI image generation models (like Stable Diffusion, DALL-E, Midjourney) compare in their ability to render hands?</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e584b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "justification, judgement = judge_classification(\n",
    "    input_content=input_content,\n",
    "    response_one=response_one,\n",
    "    response_two=response_two,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ecb8a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee2a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze both sets:\n",
      "\n",
      "Set 1:\n",
      "- Questions are clear and simple\n",
      "- However, they are quite general and basic\n",
      "- Most answers can be easily found online\n",
      "- Some questions aren't specifically related to the hand-rendering problem\n",
      "- Questions don't build much on the technical aspects mentioned in the answer\n",
      "\n",
      "Set 2:\n",
      "- Questions are more technically focused\n",
      "- Directly relates to the problem of hand rendering\n",
      "- Builds upon concepts mentioned in the answer (loss functions, rewards)\n",
      "- Asks for specific experiences and comparisons\n",
      "- Questions would be interesting to people who care about the technical aspects of AI image generation\n",
      "- Questions are challenging but still answerable by someone knowledgeable in the field\n",
      "\n",
      "The second set is better because:\n",
      "1. Questions are more specific and targeted\n",
      "2. They build upon the technical concepts mentioned in the original answer\n",
      "3. They ask for knowledge that would be valuable but isn't easily found through a simple web search\n",
      "4. The questions maintain focus on the hand-rendering problem while exploring different technical aspects\n",
      "5. They would be interesting to the same audience that was interested in the original question\n",
      "\n",
      "Neither set has ambiguous references or unnecessary words, but set 2 provides more value and depth.\n",
      "\n",
      "<label>two</label> is better.\n"
     ]
    }
   ],
   "source": [
    "print(justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d77c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "justification, judgement = judge_classification(\n",
    "    input_content=input_content,\n",
    "    response_one=response_two,  # swapped\n",
    "    response_two=response_one,  # swapped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a6a28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e2be9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze both sets:\n",
      "\n",
      "Set One:\n",
      "- Questions are specific and focused on the hand rendering problem\n",
      "- Questions build upon the original answer's discussion of rewards and loss functions\n",
      "- Questions show technical depth while remaining answerable\n",
      "- Questions are distinct and progressive\n",
      "- Questions target knowledge that would require expert experience, not just Googling\n",
      "- All questions are relevant to people interested in AI image generation and the hand problem\n",
      "\n",
      "Set Two:\n",
      "- Questions are more general and broad about AI image generation\n",
      "- Only the last question specifically relates to the hand rendering problem\n",
      "- Questions could largely be answered through basic online research\n",
      "- Questions don't deeply engage with the technical concepts introduced in the answer\n",
      "- Questions are less focused on the specific problem discussed in the original answer\n",
      "- Some questions (like \"What are some AI image generation models?\") are very basic\n",
      "\n",
      "The first set is superior because:\n",
      "1. It maintains focus on the specific problem (hand rendering)\n",
      "2. Questions require expert knowledge and experience to answer\n",
      "3. Each question builds upon concepts mentioned in the original answer\n",
      "4. Questions are technically meaningful while remaining answerable\n",
      "5. The questions would be interesting to the same audience that was interested in the original question\n",
      "\n",
      "Neither set has ambiguous references or unnecessary words, but Set One provides a more cohesive and relevant series of followup questions that better serve the interested reader.\n",
      "\n",
      "<label>one</label> is better.\n"
     ]
    }
   ],
   "source": [
    "print(justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b7b13",
   "metadata": {},
   "source": [
    "# Generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f3a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(\n",
    "    generation_prompt: str,\n",
    "    input_content: str,\n",
    ") -> str:\n",
    "    try:\n",
    "        generation_prompt_with_inputs = generation_prompt.format(\n",
    "            input_content=input_content,\n",
    "        )\n",
    "    except:\n",
    "        return \"{input_content} should appear in the prompt\"\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": generation_prompt_with_inputs\n",
    "            },\n",
    "        ],\n",
    "        stop_sequences=[\"</questions>\"],\n",
    "    )\n",
    "    message_text = message.content[0].text\n",
    "    return message_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d80ec16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation(INITIAL_GENERATION_PROMPT, input_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9616ac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<item>\n",
      "<rationale>\n",
      "Since the answer discusses training data and loss functions, readers might want to know about specific improvements in this area.\n",
      "</rationale>\n",
      "<question>\n",
      "How could AI training data be modified to better generate hands?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "The answer mentions deformities as a general problem, so readers would be interested in comparing hands to other challenging features.\n",
      "</rationale>\n",
      "<question>\n",
      "What other body parts do AI image generators struggle with?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "The technical explanation invites a question about practical solutions being developed.\n",
      "</rationale>\n",
      "<question>\n",
      "Which AI models currently generate the most accurate hands?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "The answer discusses loss functions, leading to curiosity about alternative approaches.\n",
      "</rationale>\n",
      "<question>\n",
      "What alternative loss functions could improve hand generation?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Since the answer mentions this is an open question, readers would want to know about progress.\n",
      "</rationale>\n",
      "<question>\n",
      "When will AI consistently generate realistic hands?\n",
      "</question>\n",
      "</item>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea477c",
   "metadata": {},
   "source": [
    "# Win rate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65585c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "@cache\n",
    "def calculate_winrate(\n",
    "    input_contents: tuple[str],\n",
    "    generation_prompt_one: str,\n",
    "    generation_prompt_two: str,\n",
    "    filename_suffix: str = \"\",\n",
    "):\n",
    "    one_win = 0\n",
    "    two_win = 0\n",
    "\n",
    "    one_wins_judgements = []\n",
    "    one_loses_judgements = []\n",
    "    two_wins_judgements = []\n",
    "    two_loses_judgements = []\n",
    "    one_tie_judgements = []\n",
    "    two_tie_judgements = []\n",
    "    \n",
    "    def calculate_winrate_single(input_content, index):\n",
    "        response_one = generation(\n",
    "            generation_prompt=generation_prompt_one,\n",
    "            input_content=input_content,\n",
    "        )\n",
    "        response_two = generation(\n",
    "            generation_prompt=generation_prompt_two,\n",
    "            input_content=input_content,\n",
    "        )\n",
    "        flipped = (index%2 == 1)\n",
    "        if not flipped:\n",
    "            justification, judgement = judge_classification(\n",
    "                input_content=input_content,\n",
    "                response_one=response_one,\n",
    "                response_two=response_two,\n",
    "            )\n",
    "        else:\n",
    "            justification, judgement = judge_classification(\n",
    "                input_content=input_content,\n",
    "                response_one=response_two,\n",
    "                response_two=response_one,\n",
    "            )\n",
    "        return response_one, response_two, justification, judgement, flipped\n",
    "        \n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        results = executor.map(calculate_winrate_single, input_contents, list(range(len(input_contents))))\n",
    "        results = list(results)    \n",
    "    \n",
    "    judgement_unflipped = []\n",
    "    for _, _, justification, judgement, flipped in results:\n",
    "        if not flipped:\n",
    "            if judgement == \"one\":\n",
    "                one_win += 1\n",
    "                judgement_unflipped.append(\"one\")\n",
    "                one_wins_judgements.append(justification)\n",
    "                two_loses_judgements.append(justification)\n",
    "            elif judgement == \"two\":\n",
    "                two_win += 1\n",
    "                judgement_unflipped.append(\"two\")\n",
    "                two_wins_judgements.append(justification)\n",
    "                one_loses_judgements.append(justification)\n",
    "            else:\n",
    "                one_win += 1/2\n",
    "                two_win += 1/2\n",
    "                judgement_unflipped.append(\"tie\")\n",
    "                one_tie_judgements.append(justification)\n",
    "                two_tie_judgements.append(justification)\n",
    "        else:\n",
    "            if judgement == \"two\":\n",
    "                one_win += 1\n",
    "                judgement_unflipped.append(\"one\")\n",
    "                one_wins_judgements.append(justification)\n",
    "                two_loses_judgements.append(justification)\n",
    "            elif judgement == \"one\":\n",
    "                two_win += 1\n",
    "                judgement_unflipped.append(\"two\")\n",
    "                two_wins_judgements.append(justification)\n",
    "                one_loses_judgements.append(justification)\n",
    "            else:\n",
    "                one_win += 1/2\n",
    "                two_win += 1/2\n",
    "                judgement_unflipped.append(\"tie\")\n",
    "                one_tie_judgements.append(justification)\n",
    "                two_tie_judgements.append(justification)\n",
    "    \n",
    "    if filename_suffix:\n",
    "        df_to_display = pd.DataFrame(\n",
    "            {\n",
    "                \"input_content\": [\"\"] + list(input_contents),\n",
    "                \"response_one\": [generation_prompt_one] + [response_one for response_one, _, _, _, _ in results],\n",
    "                \"response_two\": [generation_prompt_two] + [response_two for _, response_two, _, _, _ in results],\n",
    "                \"judgement\": [\"\"] + judgement_unflipped,\n",
    "                \"justification\": [\"\"] + [justification for _, _, justification, _, _ in results],\n",
    "            }\n",
    "        )\n",
    "        display_dataframe(df_to_display, filename_suffix=filename_suffix)\n",
    "        \n",
    "    return (\n",
    "        (\n",
    "            one_win / (one_win + two_win),\n",
    "            one_wins_judgements,\n",
    "            one_tie_judgements,\n",
    "            one_loses_judgements,\n",
    "        ),\n",
    "        (\n",
    "            two_win / (one_win + two_win),\n",
    "            two_wins_judgements,\n",
    "            two_tie_judgements,\n",
    "            two_loses_judgements,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b9a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import html\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_dataframe(df: pd.DataFrame, filename_suffix=\"\"):\n",
    "    html_prefix = '''\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "    }\n",
    "    td, th {\n",
    "        border: 1px solid black;\n",
    "        padding: 5px;\n",
    "        vertical-align: top;\n",
    "    }\n",
    "    td {\n",
    "        white-space: pre-wrap;\n",
    "        font-family: monospace;\n",
    "    }\n",
    "    </style>\n",
    "    '''\n",
    "    \n",
    "    # Define a single style function that highlights response_one or response_two based on judgement\n",
    "    def highlight_responses(row):\n",
    "        styles = ['' for _ in row]\n",
    "        if row['judgement'] == 'one':\n",
    "            styles[row.index.get_loc('response_one')] = 'background-color: #90EE90'\n",
    "        elif row['judgement'] == 'two':\n",
    "            styles[row.index.get_loc('response_two')] = 'background-color: #90EE90'\n",
    "        return styles\n",
    "\n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    output_table_file_name = f\"html_output/winrate_calculation{filename_suffix}.html\"\n",
    "    \n",
    "    # Replace newline characters and escape HTML\n",
    "    styled_df = df.replace({r'\\n': '__NEWLINE__'}, regex=True).applymap(str).applymap(html.escape).replace({'__NEWLINE__': '<br>'}, regex=True)\n",
    "    \n",
    "    # Apply the style function\n",
    "    styled_df = styled_df.style.apply(highlight_responses, axis=1)\n",
    "    \n",
    "    # Write the styled DataFrame to an HTML file\n",
    "    with open(output_table_file_name, 'w') as f:\n",
    "        f.write(html_prefix + styled_df.render(index=False, escape=False))\n",
    "    \n",
    "    # Create a clickable link to the HTML file\n",
    "    link = f'<a href=\"{output_table_file_name}\" target=\"_blank\">{output_table_file_name}</a>'\n",
    "    display(HTML(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92be4a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-demo.html\" target=\"_blank\">html_output/winrate_calculation-demo.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_one, evaluation_two = calculate_winrate(\n",
    "    input_contents = INPUT_CONTENTS[:20],\n",
    "    generation_prompt_one = INITIAL_GENERATION_PROMPT,\n",
    "    generation_prompt_two = \"Generate a question and return in <question> and </question>\",\n",
    "    filename_suffix = \"-demo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c4eee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9943820224719101, [\"Let me analyze both sets:\\n\\nSet 1:\\n- All questions are directly related to the source topic\\n- Questions are clear and concise\\n- No ambiguous references\\n- Questions are distinct from each other\\n- Questions address technical aspects that would interest someone reading about AI image generation\\n- The questions logically follow from the explanation about loss functions and training\\n- The author, being a Machine Learning Engineer, would likely be qualified to answer the\n"
     ]
    }
   ],
   "source": [
    "print(str(evaluation_one)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabd546",
   "metadata": {},
   "source": [
    "# Prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9abc71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_string_template = \"\"\"\n",
    "Winrate: {winrate}\n",
    "Cases where the prompt won: {win_judgements}\n",
    "Cases where the prompt ties: {tie_judgements}\n",
    "Cases where the prompt loses: {lose_judgements}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc7f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_prompt_template = \"\"\"\n",
    "Improve the generation prompt according to the feedback\n",
    "\n",
    "<current_generation_prompt>\n",
    "{generation_prompt}\n",
    "</current_generation_prompt>\n",
    "\n",
    "<feedback>\n",
    "{evaluation_string}\n",
    "</feedback>\n",
    "\n",
    "This is the judging criteria\n",
    "<judging_criteria>\n",
    "{judging_criteria}\n",
    "</judging_criteria>\n",
    "\n",
    "Summarize the changes that you intend to make, and return the new prompt between <prompt> and </prompt>.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c644c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(\n",
    "    generation_prompt: str,\n",
    "    evaluations: list, \n",
    ") -> str:\n",
    "    evaluation_string = \"\"\n",
    "    for evaluation in evaluations:\n",
    "        winrate, win_judgements, tie_judgements, lose_judgements = evaluation\n",
    "        evaluation_string_single = evaluation_string_template.format(\n",
    "            winrate=winrate,\n",
    "            win_judgements=win_judgements,\n",
    "            tie_judgements=tie_judgements,\n",
    "            lose_judgements=lose_judgements,\n",
    "        )\n",
    "        evaluation_string += evaluation_string_single\n",
    "\n",
    "    optimization_prompt = optimization_prompt_template.format(\n",
    "        generation_prompt=generation_prompt,\n",
    "        evaluation_string=evaluation_string,\n",
    "        judging_criteria=JUDGE_CLASSIFICATION_PROMPT,\n",
    "    )\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": optimization_prompt\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    optimization_response = message.content[0].text\n",
    "    optimized_prompt = extract_from_tags(optimization_response, tag_string=\"prompt\")\n",
    "    \n",
    "    return optimization_response, optimized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e77b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_from_tags(text, tag_string=\"prompt\"):\n",
    "    pattern = f'<{tag_string}>(.*?)</{tag_string}>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dafa2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_response, optimized_prompt = optimization(INITIAL_GENERATION_PROMPT, [evaluation_one])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db3da27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the feedback, I notice these key patterns for successful prompts:\n",
      "\n",
      "1. The winning sets consistently:\n",
      "- Maintained topic relevance\n",
      "- Had exactly 5 questions\n",
      "- Asked distinct questions\n",
      "- Used concise language without ambiguity\n",
      "- Built naturally from the source content\n",
      "- Asked for knowledge requiring expertise/experience\n",
      "- Would interest the original audience\n",
      "- Could be reasonably answered by the author\n",
      "\n",
      "2. The losing sets typically:\n",
      "- Were completely off-topic\n",
      "- Had only 1 question instead of 5\n",
      "- Asked about random topics unrelated to the source\n",
      "- Could be easily googled\n",
      "\n",
      "Changes I'll make to improve the prompt:\n",
      "\n",
      "1. Add explicit requirements for topical relevance\n",
      "2. Emphasize that questions should build upon the source content\n",
      "3. Add clarity about avoiding easily googleable questions\n",
      "4. Stress the importance of asking questions the author could reasonably answer\n",
      "5. Maintain the XML structure for consistency\n",
      "6. Keep the focus on practical, experience-based knowledge\n",
      "\n",
      "Here's the i\n"
     ]
    }
   ],
   "source": [
    "print(optimization_response[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c94a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Quora answer. Write 5 followup questions to this answer.\n",
      "\n",
      "Requirements:\n",
      "- Questions must be directly related to the source content\n",
      "- Questions should build naturally from information provided in the answer\n",
      "- Questions should ask for knowledge that requires expertise or personal experience (not easily googleable)\n",
      "- Questions should be ones that the author would be qualified to answer based on their demonstrated knowledge\n",
      "- Questions should interest people who read the original answer\n",
      "- Questions must be clear without requiring additional context\n",
      "- Questions should use minimal necessary words\n",
      "- Each question must be distinct from the others\n",
      "\n",
      "Reply in this format:\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Why this question follows from the source and what makes it valuable\n",
      "</rationale>\n",
      "<question>\n",
      "The actual question\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "[Repeat for all 5 questions]\n",
      "\n",
      "This is the question and answer:\n",
      "\n",
      "<source_question_and_answer>\n",
      "{input_content}\n",
      "</source_question_and_answer>\n"
     ]
    }
   ],
   "source": [
    "print(optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7ce0",
   "metadata": {},
   "source": [
    "# Iterative optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62555865",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompts = [INITIAL_GENERATION_PROMPT, INITIAL_GENERATION_PROMPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5988573b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-0-1.html\" target=\"_blank\">html_output/winrate_calculation-0-1.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Quora answer. Write 5 followup questions to this answer.\n",
      "\n",
      "Requirements:\n",
      "- Questions must be specific and focused on unique aspects mentioned in the answer\n",
      "- Questions should ask for personal experiences and insights not easily found online\n",
      "- Questions should follow a logical progression and build upon each other\n",
      "- Questions should focus on practical implications where appropriate\n",
      "- Questions must be completely understandable without additional context\n",
      "- Questions should not use more words than necessary\n",
      "- Questions must be ones the author would be able to answer based on their demonstrated knowledge\n",
      "- Questions must maintain thematic consistency with the original answer\n",
      "- Each question should be distinct from the others\n",
      "\n",
      "Reply in this format:\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "rationale\n",
      "</rationale>\n",
      "<question>\n",
      "question\n",
      "</question>\n",
      "</item>\n",
      "...\n",
      "\n",
      "This is the question and answer:\n",
      "\n",
      "<source_question_and_answer>\n",
      "{input_content}\n",
      "</source_question_and_answer>\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-0-2.html\" target=\"_blank\">html_output/winrate_calculation-0-2.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-1-2.html\" target=\"_blank\">html_output/winrate_calculation-1-2.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Quora answer. Write 5 followup questions to this answer.\n",
      "\n",
      "Requirements:\n",
      "- Questions must be specific and focused on unique aspects mentioned in the answer\n",
      "- Questions should ask for insights and experiences that aren't easily found online\n",
      "- Questions should follow a logical progression and build upon each other\n",
      "- Questions should balance specific details with broader implications where appropriate\n",
      "- Questions must be completely understandable without additional context\n",
      "- Questions should be appropriately detailed while avoiding unnecessary words\n",
      "- Questions must not make assumptions about the author's role or involvement\n",
      "- Questions must align with the demonstrated knowledge in the answer\n",
      "- Questions must maintain thematic consistency with the original answer\n",
      "- Each question should be distinct from the others\n",
      "- Questions should maintain an appropriate scope - neither too narrow nor too broad\n",
      "\n",
      "Reply in this format:\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "rationale\n",
      "</rationale>\n",
      "<question>\n",
      "question\n",
      "</question>\n",
      "</item>\n",
      "...\n",
      "\n",
      "This is the question and answer:\n",
      "\n",
      "<source_question_and_answer>\n",
      "{input_content}\n",
      "</source_question_and_answer>\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-0-3.html\" target=\"_blank\">html_output/winrate_calculation-0-3.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-1-3.html\" target=\"_blank\">html_output/winrate_calculation-1-3.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-2-3.html\" target=\"_blank\">html_output/winrate_calculation-2-3.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Quora answer. Write 5 followup questions to this answer.\n",
      "\n",
      "Requirements:\n",
      "- Questions must be maximally concise while maintaining clarity\n",
      "- Questions must directly reference specific points or examples from the answer\n",
      "- Questions should ask for personal experiences and practical insights\n",
      "- Questions must be answerable based on the expertise demonstrated in the answer\n",
      "- Questions should maintain a natural, conversational tone\n",
      "- Questions must be completely understandable without additional context\n",
      "- Questions must seek information that isn't easily found through online searches\n",
      "- Questions should build naturally from the themes in the original answer\n",
      "- Each question must be distinct and focused on a single aspect\n",
      "- Questions should avoid theoretical or academic abstractions in favor of practical applications\n",
      "\n",
      "Reply in this format:\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "rationale\n",
      "</rationale>\n",
      "<question>\n",
      "question\n",
      "</question>\n",
      "</item>\n",
      "...\n",
      "\n",
      "This is the question and answer:\n",
      "\n",
      "<source_question_and_answer>\n",
      "{input_content}\n",
      "</source_question_and_answer>\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for latest_index in range(1, 4):\n",
    "    generation_prompt_latest = generation_prompts[-1]\n",
    "    evaluations = []\n",
    "    for previous_index, generation_prompt_old in enumerate(generation_prompts[:-1]):\n",
    "        random.seed(f\"{previous_index} {latest_index}\")\n",
    "        input_contents = tuple(random.sample(INPUT_CONTENTS, min(20, len(INPUT_CONTENTS))))\n",
    "        _, evaluation = calculate_winrate(\n",
    "            input_contents,\n",
    "            generation_prompt_old,\n",
    "            generation_prompt_latest,\n",
    "            filename_suffix = f\"-{previous_index}-{latest_index}\",\n",
    "        )\n",
    "        evaluations.append(evaluation)\n",
    "\n",
    "    optimization_response, generation_prompt_new = optimization(generation_prompt_latest, evaluations)\n",
    "    generation_prompts.append(generation_prompt_new)\n",
    "    print(generation_prompt_new)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471a0d6",
   "metadata": {},
   "source": [
    "# Display winrate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cf806e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-0-4.html\" target=\"_blank\">html_output/winrate_calculation-0-4.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-1-4.html\" target=\"_blank\">html_output/winrate_calculation-1-4.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-2-4.html\" target=\"_blank\">html_output/winrate_calculation-2-4.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/winrate_calculation-3-4.html\" target=\"_blank\">html_output/winrate_calculation-3-4.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "win_rate_matrix = [[np.nan for _ in generation_prompts] for _ in generation_prompts]\n",
    "\n",
    "for one_idx, prompt_one in enumerate(generation_prompts):\n",
    "    for two_idx, prompt_two in enumerate(generation_prompts[one_idx+1:], start=one_idx+1):\n",
    "        random.seed(f\"{one_idx} {two_idx}\")\n",
    "        input_contents = tuple(random.sample(INPUT_CONTENTS, min(20, len(INPUT_CONTENTS))))\n",
    "        evaluation_one, evaluation_two = calculate_winrate(\n",
    "            input_contents,\n",
    "            prompt_one,\n",
    "            prompt_two,\n",
    "            filename_suffix = f\"-{one_idx}-{two_idx}\",\n",
    "        )\n",
    "        win_rate_one, _, _, _ = evaluation_one\n",
    "        win_rate_two, _, _, _ = evaluation_two\n",
    "        win_rate_matrix[two_idx][one_idx] = win_rate_one\n",
    "        win_rate_matrix[one_idx][two_idx] = win_rate_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cca8721a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[nan, 0.4, 0.75, 0.6, 0.775],\n",
       " [0.6, nan, 0.7, 0.6, 0.65],\n",
       " [0.25, 0.3, nan, 0.1, 0.6],\n",
       " [0.4, 0.4, 0.9, nan, 0.65],\n",
       " [0.225, 0.35, 0.4, 0.35, nan]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_rate_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32d1d118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1' style='border-collapse: collapse;'><tr><th></th><th>Prompt 0</th><th>Prompt 1</th><th>Prompt 2</th><th>Prompt 3</th><th>Prompt 4</th></tr><tr><th>Prompt 0</th><td></td><td><a href=\"html_output/winrate_calculation-0-1.html\" target=\"_blank\" style=\"text-decoration: none;\">0.40</a></td><td><a href=\"html_output/winrate_calculation-0-2.html\" target=\"_blank\" style=\"text-decoration: none;\">0.75</a></td><td><a href=\"html_output/winrate_calculation-0-3.html\" target=\"_blank\" style=\"text-decoration: none;\">0.60</a></td><td><a href=\"html_output/winrate_calculation-0-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.78</a></td></tr><tr><th>Prompt 1</th><td><a href=\"html_output/winrate_calculation-0-1.html\" target=\"_blank\" style=\"text-decoration: none;\">0.60</a></td><td></td><td><a href=\"html_output/winrate_calculation-1-2.html\" target=\"_blank\" style=\"text-decoration: none;\">0.70</a></td><td><a href=\"html_output/winrate_calculation-1-3.html\" target=\"_blank\" style=\"text-decoration: none;\">0.60</a></td><td><a href=\"html_output/winrate_calculation-1-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.65</a></td></tr><tr><th>Prompt 2</th><td><a href=\"html_output/winrate_calculation-0-2.html\" target=\"_blank\" style=\"text-decoration: none;\">0.25</a></td><td><a href=\"html_output/winrate_calculation-1-2.html\" target=\"_blank\" style=\"text-decoration: none;\">0.30</a></td><td></td><td><a href=\"html_output/winrate_calculation-2-3.html\" target=\"_blank\" style=\"text-decoration: none;\">0.10</a></td><td><a href=\"html_output/winrate_calculation-2-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.60</a></td></tr><tr><th>Prompt 3</th><td><a href=\"html_output/winrate_calculation-0-3.html\" target=\"_blank\" style=\"text-decoration: none;\">0.40</a></td><td><a href=\"html_output/winrate_calculation-1-3.html\" target=\"_blank\" style=\"text-decoration: none;\">0.40</a></td><td><a href=\"html_output/winrate_calculation-2-3.html\" target=\"_blank\" style=\"text-decoration: none;\">0.90</a></td><td></td><td><a href=\"html_output/winrate_calculation-3-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.65</a></td></tr><tr><th>Prompt 4</th><td><a href=\"html_output/winrate_calculation-0-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.23</a></td><td><a href=\"html_output/winrate_calculation-1-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.35</a></td><td><a href=\"html_output/winrate_calculation-2-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.40</a></td><td><a href=\"html_output/winrate_calculation-3-4.html\" target=\"_blank\" style=\"text-decoration: none;\">0.35</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "num_prompts = len(generation_prompts)\n",
    "\n",
    "# Start building the HTML table\n",
    "html_table = \"<table border='1' style='border-collapse: collapse;'>\"\n",
    "\n",
    "# Create the header row\n",
    "html_table += \"<tr><th></th>\"\n",
    "for j in range(num_prompts):\n",
    "    html_table += f\"<th>Prompt {j}</th>\"\n",
    "html_table += \"</tr>\"\n",
    "\n",
    "for i in range(num_prompts):\n",
    "    html_table += f\"<tr><th>Prompt {i}</th>\"\n",
    "    for j in range(num_prompts):\n",
    "        if i == j or np.isnan(win_rate_matrix[i][j]):\n",
    "            html_table += \"<td></td>\"  # Empty cell for diagonal or undefined win rates\n",
    "        else:\n",
    "            win_rate = win_rate_matrix[i][j]\n",
    "            cell_html = f'<a href=\"html_output/winrate_calculation-{min(i,j)}-{max(i,j)}.html\" target=\"_blank\" style=\"text-decoration: none;\">{win_rate:.2f}</a>'\n",
    "            html_table += f\"<td>{cell_html}</td>\"\n",
    "    html_table += \"</tr>\"\n",
    "html_table += \"</table>\"\n",
    "\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af4883",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "You notice that the prompts keeps getting better, the each prompt is better than the previous prompt, according the the judge we assume we trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9cfec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do AI image generators have so much trouble rendering hands?\n",
      "\n",
      "Tong Hui Kang\n",
      "Machine Learning Engineer in Recommendations\n",
      "The current AI image generators are not rewarded for drawing hands well.\n",
      "The training input for image models is a huge set of images and their captions. The objective here is to fit a function f(caption) -> image, by minimizing some loss function L(caption, image).\n",
      "L(caption, image) is high when the image is unrelated to the caption, and L(caption, image) is low when the image is very related to the caption. The image generation model is trained to minimize L(caption, image).\n",
      "However, L(caption, image) might be high for images with deformities, because L(caption, image) might only measure whether the image is related to the caption.\n",
      "If you don’t penalize deformities in the training, you can’t generate images free-of-deformities. As of my current understanding, generating images without deformities is still an open question.\n"
     ]
    }
   ],
   "source": [
    "print(input_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d017c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation(generation_prompts[0], input_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0af194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<item>\n",
      "<rationale>\n",
      "Asking about the technical details of the loss function mentioned would help readers understand how AI image quality is measured.\n",
      "</rationale>\n",
      "<question>\n",
      "How exactly do loss functions measure image-caption relationships?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Since the answer mentions that deformities aren't penalized, readers would want to know if there are potential solutions.\n",
      "</rationale>\n",
      "<question>\n",
      "What methods could penalize anatomical deformities in AI image generation?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "The answer focuses on hands, but readers would wonder about other common AI image generation problems.\n",
      "</rationale>\n",
      "<question>\n",
      "What other body parts do AI image generators struggle with?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Since training data is mentioned as crucial, readers would be interested in data quality impacts.\n",
      "</rationale>\n",
      "<question>\n",
      "How does training data quality affect hand rendering?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "The answer suggests this is an open problem, so readers would want to know about progress.\n",
      "</rationale>\n",
      "<question>\n",
      "What recent improvements have been made in AI hand generation?\n",
      "</question>\n",
      "</item>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e5aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation(generation_prompts[-1], input_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e5abb39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<item>\n",
      "<rationale>\n",
      "Asks about practical experience with loss functions, focusing on how real implementations handle specific image quality issues.\n",
      "</rationale>\n",
      "<question>\n",
      "What specific loss functions have you found most effective at catching anatomical errors in generated images?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Explores the practical challenges of implementing deformity detection in training, drawing on the respondent's hands-on experience.\n",
      "</rationale>\n",
      "<question>\n",
      "How do you balance penalizing deformities against maintaining creative freedom in your image generation models?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Seeks insights about real-world solutions to the caption-image relationship problem mentioned in the answer.\n",
      "</rationale>\n",
      "<question>\n",
      "What caption preprocessing techniques help improve anatomical accuracy in your generated images?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Builds on the answer's discussion of training objectives by asking about practical metrics for success.\n",
      "</rationale>\n",
      "<question>\n",
      "How do you measure and track improvements in hand rendering quality across model iterations?\n",
      "</question>\n",
      "</item>\n",
      "\n",
      "<item>\n",
      "<rationale>\n",
      "Explores the practical implications of the \"open question\" mentioned in the answer through real-world experience.\n",
      "</rationale>\n",
      "<question>\n",
      "What promising approaches have you seen for detecting and correcting anatomical errors during generation?\n",
      "</question>\n",
      "</item>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062051d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
